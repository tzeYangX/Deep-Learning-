{"cells":[{"cell_type":"markdown","metadata":{"id":"D0A8172D4DC04B9C8E4ECE8C893C9096","mdEditEnable":false},"source":["# 利用神经网络解决手写数字问题\n","python3 pytorch0.4.0"],"hide_input":false,"execution_count":null},{"cell_type":"markdown","metadata":{"id":"4F09D88DC71D4693875F124A4F093D25","mdEditEnable":false},"source":["1.载入数据集"],"hide_input":false,"execution_count":null,"outputs":[]},{"metadata":{"id":"B1C963FDA9CB4BF38F132C7D55A56D1B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting torch==0.4.0\n\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/33/65/e40bc02754ad2fd66e4953e276d8c608cdb0e3b6b516fb485ea1efaeeeea/torch-0.4.0-cp35-cp35m-manylinux1_x86_64.whl (484.0MB)\n\u001b[K    100% |████████████████████████████████| 484.0MB 114kB/s \n\u001b[?25hInstalling collected packages: torch\n  Found existing installation: torch 0.3.1\n    Uninstalling torch-0.3.1:\n      Successfully uninstalled torch-0.3.1\nSuccessfully installed torch-0.4.0\n","name":"stdout"}],"hide_input":false,"source":["#!conda list\n","!pip install torch==0.4.0"],"execution_count":1},{"metadata":{"id":"6BF20318F81542858B7DCB34516A71E0","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"processed  raw\r\n","name":"stdout"}],"hide_input":false,"source":["!ls /home/kesci/input/fashion_mnist5774/fashion-mnist/mnist"],"execution_count":1},{"cell_type":"code","execution_count":8,"metadata":{"id":"42FE412D115B48B599AFB336DB55EEE2","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Dataset FashionMNIST\n    Number of datapoints: 60000\n    Split: train\n    Root Location: /home/kesci/input/fashion_mnist5774/fashion-mnist/fashion\n    Transforms (if any): Compose(\n                             ToTensor()\n                             Normalize(mean=(0.1307,), std=(0.3081,))\n                         )\n    Target Transforms (if any): None\nDataset FashionMNIST\n    Number of datapoints: 10000\n    Split: test\n    Root Location: /home/kesci/input/fashion_mnist5774/fashion-mnist/fashion\n    Transforms (if any): Compose(\n                             ToTensor()\n                             Normalize(mean=(0.1307,), std=(0.3081,))\n                         )\n    Target Transforms (if any): None\ntorch.Size([1000, 1, 28, 28])\n","name":"stdout"},{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 288x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/42FE412D115B48B599AFB336DB55EEE2/patf3kkw9t.png\">"}}],"source":["# PyTorch MNIST Example 参见https://github.com/pytorch/examples\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import pylab\n","\n","base_dir='/home/kesci/input/fashion_mnist5774/fashion-mnist/'\n","train_set = datasets.FashionMNIST(  # \n","    base_dir+'fashion',  # 数据集位置\n","    train=True,  # 训练集\n","    download=False,  # 下载\n","    transform=transforms.Compose(  # 变换\n","        [\n","            transforms.ToTensor(),  # 从[0,255]归一化到[0,1]\n","            transforms.Normalize((0.1307, ), (0.3081, ))\n","        ]))  # 进一步按照均值方差归一化\n","test_set = datasets.FashionMNIST(\n","    base_dir+'fashion',\n","    train=False,  # 测试集\n","    transform=transforms.Compose(\n","        [transforms.ToTensor(),\n","         transforms.Normalize((0.1307, ), (0.3081, ))]))\n","train_loader = torch.utils.data.DataLoader(  # 提取器\n","    train_set, batch_size=100, shuffle=True)  # batch_size 批处理大小 shuffle 是否打乱顺序\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_set, batch_size=1000, shuffle=False)\n","\n","pylab.rcParams['figure.figsize'] = (4.0, 4.0) # plot size\n","print(train_set)\n","print(test_set)\n","img, lbl = iter(test_loader).next()  # 深度学习框架一般按照[n c h w]的顺序排列数据\n","print(img.shape)\n","img = img[0, :, :].data.numpy().squeeze(0)  # 转化为numpy格式并去除额外维度\n","plt.imshow(img, cmap='jet')\n","plt.show()"],"hide_input":false},{"cell_type":"markdown","metadata":{"id":"C1114A22CED24695AF0346F17241729A","mdEditEnable":false},"source":["2.设计神经网络"],"hide_input":false,"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"BD60A37C37A1472F89D666301CD982E5","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"LeNet5(\n  (C1): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n  )\n  (S2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (C3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n  )\n  (S4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (C5): Sequential(\n    (0): Linear(in_features=1600, out_features=256, bias=True)\n    (1): ReLU()\n  )\n  (F6): Sequential(\n    (0): Linear(in_features=256, out_features=128, bias=True)\n    (1): ReLU()\n  )\n  (OUT): Linear(in_features=128, out_features=16, bias=True)\n)\n","name":"stdout"}],"source":["class LeNet5(nn.Module):  # 参考lenet5网络结构 形似\n","    def __init__(self):\n","        super(LeNet5, self).__init__()\n","        self.C1 = nn.Sequential(nn.Conv2d(1, 32, kernel_size=5), nn.ReLU())  # 卷积层 + ReLU 激活函数\n","        self.S2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 最大值池化\n","        self.C3 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3), nn.ReLU())\n","        self.S4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.C5 = nn.Sequential(nn.Linear(1600, 256), nn.ReLU())  # 全连接层\n","        self.F6 = nn.Sequential(nn.Linear(256, 128), nn.ReLU())\n","        self.OUT = nn.Linear(128, 16)\n","\n","    def forward(self, x):\n","        x = self.C1(x)\n","        x = self.S2(x)\n","        x = self.C3(x)\n","        x = self.S4(x)\n","        x = x.view(-1, 1600)  # 将图展开成向量\n","        x = self.C5(x)\n","        x = self.F6(x)\n","        x = self.OUT(x)\n","        return F.log_softmax(x, dim=1)  # softmax输出 用于分类任务\n","print(LeNet5())"],"hide_input":false},{"cell_type":"markdown","metadata":{"id":"9D82BA2214CC4D498AAB1AE402527341","mdEditEnable":false},"source":["3.训练神经网络"],"hide_input":false,"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"3A87A7C78F164EA9969E55C7EAFC706B","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.782146\nTrain Epoch: 1 [10000/60000 (17%)]\tLoss: 0.830961\nTrain Epoch: 1 [20000/60000 (33%)]\tLoss: 0.613747\nTrain Epoch: 1 [30000/60000 (50%)]\tLoss: 0.527018\nTrain Epoch: 1 [40000/60000 (67%)]\tLoss: 0.627650\nTrain Epoch: 1 [50000/60000 (83%)]\tLoss: 0.464296\n\nTest set: Average loss: 0.4043, Accuracy: 8499/10000 (84.9900%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.431176\nTrain Epoch: 2 [10000/60000 (17%)]\tLoss: 0.477238\nTrain Epoch: 2 [20000/60000 (33%)]\tLoss: 0.401102\nTrain Epoch: 2 [30000/60000 (50%)]\tLoss: 0.269246\nTrain Epoch: 2 [40000/60000 (67%)]\tLoss: 0.317353\nTrain Epoch: 2 [50000/60000 (83%)]\tLoss: 0.263304\n\nTest set: Average loss: 0.3617, Accuracy: 8701/10000 (87.0100%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.348743\nTrain Epoch: 3 [10000/60000 (17%)]\tLoss: 0.365958\nTrain Epoch: 3 [20000/60000 (33%)]\tLoss: 0.257758\nTrain Epoch: 3 [30000/60000 (50%)]\tLoss: 0.216328\nTrain Epoch: 3 [40000/60000 (67%)]\tLoss: 0.330117\nTrain Epoch: 3 [50000/60000 (83%)]\tLoss: 0.375021\n\nTest set: Average loss: 0.3144, Accuracy: 8834/10000 (88.3400%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.411854\nTrain Epoch: 4 [10000/60000 (17%)]\tLoss: 0.189593\nTrain Epoch: 4 [20000/60000 (33%)]\tLoss: 0.252718\nTrain Epoch: 4 [30000/60000 (50%)]\tLoss: 0.136641\nTrain Epoch: 4 [40000/60000 (67%)]\tLoss: 0.207163\nTrain Epoch: 4 [50000/60000 (83%)]\tLoss: 0.076894\n\nTest set: Average loss: 0.2904, Accuracy: 8942/10000 (89.4200%)\n\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.249372\nTrain Epoch: 5 [10000/60000 (17%)]\tLoss: 0.249567\nTrain Epoch: 5 [20000/60000 (33%)]\tLoss: 0.199222\nTrain Epoch: 5 [30000/60000 (50%)]\tLoss: 0.181554\nTrain Epoch: 5 [40000/60000 (67%)]\tLoss: 0.309977\nTrain Epoch: 5 [50000/60000 (83%)]\tLoss: 0.244052\n\nTest set: Average loss: 0.2768, Accuracy: 8941/10000 (89.4100%)\n\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 0.255980\nTrain Epoch: 6 [10000/60000 (17%)]\tLoss: 0.298397\nTrain Epoch: 6 [20000/60000 (33%)]\tLoss: 0.149175\nTrain Epoch: 6 [30000/60000 (50%)]\tLoss: 0.178420\nTrain Epoch: 6 [40000/60000 (67%)]\tLoss: 0.297109\nTrain Epoch: 6 [50000/60000 (83%)]\tLoss: 0.190502\n\nTest set: Average loss: 0.2678, Accuracy: 9052/10000 (90.5200%)\n\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.268616\nTrain Epoch: 7 [10000/60000 (17%)]\tLoss: 0.313519\nTrain Epoch: 7 [20000/60000 (33%)]\tLoss: 0.231875\nTrain Epoch: 7 [30000/60000 (50%)]\tLoss: 0.401110\nTrain Epoch: 7 [40000/60000 (67%)]\tLoss: 0.167993\nTrain Epoch: 7 [50000/60000 (83%)]\tLoss: 0.157069\n\nTest set: Average loss: 0.2477, Accuracy: 9097/10000 (90.9700%)\n\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.144281\nTrain Epoch: 8 [10000/60000 (17%)]\tLoss: 0.202210\nTrain Epoch: 8 [20000/60000 (33%)]\tLoss: 0.160528\nTrain Epoch: 8 [30000/60000 (50%)]\tLoss: 0.161919\nTrain Epoch: 8 [40000/60000 (67%)]\tLoss: 0.305975\nTrain Epoch: 8 [50000/60000 (83%)]\tLoss: 0.150329\n\nTest set: Average loss: 0.2457, Accuracy: 9112/10000 (91.1200%)\n\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.112108\nTrain Epoch: 9 [10000/60000 (17%)]\tLoss: 0.178293\nTrain Epoch: 9 [20000/60000 (33%)]\tLoss: 0.193562\nTrain Epoch: 9 [30000/60000 (50%)]\tLoss: 0.139868\nTrain Epoch: 9 [40000/60000 (67%)]\tLoss: 0.207425\nTrain Epoch: 9 [50000/60000 (83%)]\tLoss: 0.198190\n\nTest set: Average loss: 0.2511, Accuracy: 9121/10000 (91.2100%)\n\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.192484\nTrain Epoch: 10 [10000/60000 (17%)]\tLoss: 0.211500\nTrain Epoch: 10 [20000/60000 (33%)]\tLoss: 0.286356\nTrain Epoch: 10 [30000/60000 (50%)]\tLoss: 0.105730\nTrain Epoch: 10 [40000/60000 (67%)]\tLoss: 0.203636\nTrain Epoch: 10 [50000/60000 (83%)]\tLoss: 0.334686\n\nTest set: Average loss: 0.2483, Accuracy: 9139/10000 (91.3900%)\n\nTrain Epoch: 11 [0/60000 (0%)]\tLoss: 0.199228\nTrain Epoch: 11 [10000/60000 (17%)]\tLoss: 0.087921\nTrain Epoch: 11 [20000/60000 (33%)]\tLoss: 0.226354\nTrain Epoch: 11 [30000/60000 (50%)]\tLoss: 0.104834\nTrain Epoch: 11 [40000/60000 (67%)]\tLoss: 0.129830\nTrain Epoch: 11 [50000/60000 (83%)]\tLoss: 0.263216\n\nTest set: Average loss: 0.2508, Accuracy: 9110/10000 (91.1000%)\n\nTrain Epoch: 12 [0/60000 (0%)]\tLoss: 0.064001\nTrain Epoch: 12 [10000/60000 (17%)]\tLoss: 0.108482\nTrain Epoch: 12 [20000/60000 (33%)]\tLoss: 0.103653\nTrain Epoch: 12 [30000/60000 (50%)]\tLoss: 0.111880\nTrain Epoch: 12 [40000/60000 (67%)]\tLoss: 0.153459\nTrain Epoch: 12 [50000/60000 (83%)]\tLoss: 0.145018\n\nTest set: Average loss: 0.2471, Accuracy: 9181/10000 (91.8100%)\n\nTrain Epoch: 13 [0/60000 (0%)]\tLoss: 0.091942\nTrain Epoch: 13 [10000/60000 (17%)]\tLoss: 0.166189\nTrain Epoch: 13 [20000/60000 (33%)]\tLoss: 0.120512\nTrain Epoch: 13 [30000/60000 (50%)]\tLoss: 0.118235\nTrain Epoch: 13 [40000/60000 (67%)]\tLoss: 0.168736\nTrain Epoch: 13 [50000/60000 (83%)]\tLoss: 0.111270\n\nTest set: Average loss: 0.2532, Accuracy: 9181/10000 (91.8100%)\n\nTrain Epoch: 14 [0/60000 (0%)]\tLoss: 0.075852\nTrain Epoch: 14 [10000/60000 (17%)]\tLoss: 0.077567\nTrain Epoch: 14 [20000/60000 (33%)]\tLoss: 0.037225\nTrain Epoch: 14 [30000/60000 (50%)]\tLoss: 0.082353\nTrain Epoch: 14 [40000/60000 (67%)]\tLoss: 0.096833\nTrain Epoch: 14 [50000/60000 (83%)]\tLoss: 0.060707\n\nTest set: Average loss: 0.2568, Accuracy: 9169/10000 (91.6900%)\n\nTrain Epoch: 15 [0/60000 (0%)]\tLoss: 0.054403\nTrain Epoch: 15 [10000/60000 (17%)]\tLoss: 0.127570\nTrain Epoch: 15 [20000/60000 (33%)]\tLoss: 0.147309\nTrain Epoch: 15 [30000/60000 (50%)]\tLoss: 0.109187\nTrain Epoch: 15 [40000/60000 (67%)]\tLoss: 0.031999\nTrain Epoch: 15 [50000/60000 (83%)]\tLoss: 0.160151\n\nTest set: Average loss: 0.2676, Accuracy: 9131/10000 (91.3100%)\n\nTrain Epoch: 16 [0/60000 (0%)]\tLoss: 0.056429\nTrain Epoch: 16 [10000/60000 (17%)]\tLoss: 0.051309\nTrain Epoch: 16 [20000/60000 (33%)]\tLoss: 0.063511\nTrain Epoch: 16 [30000/60000 (50%)]\tLoss: 0.082439\nTrain Epoch: 16 [40000/60000 (67%)]\tLoss: 0.121039\nTrain Epoch: 16 [50000/60000 (83%)]\tLoss: 0.060305\n\nTest set: Average loss: 0.2970, Accuracy: 9174/10000 (91.7400%)\n\nTrain Epoch: 17 [0/60000 (0%)]\tLoss: 0.055047\nTrain Epoch: 17 [10000/60000 (17%)]\tLoss: 0.038707\nTrain Epoch: 17 [20000/60000 (33%)]\tLoss: 0.076220\nTrain Epoch: 17 [30000/60000 (50%)]\tLoss: 0.036235\nTrain Epoch: 17 [40000/60000 (67%)]\tLoss: 0.055696\nTrain Epoch: 17 [50000/60000 (83%)]\tLoss: 0.082099\n\nTest set: Average loss: 0.2990, Accuracy: 9147/10000 (91.4700%)\n\nTrain Epoch: 18 [0/60000 (0%)]\tLoss: 0.038811\nTrain Epoch: 18 [10000/60000 (17%)]\tLoss: 0.038507\nTrain Epoch: 18 [20000/60000 (33%)]\tLoss: 0.035873\nTrain Epoch: 18 [30000/60000 (50%)]\tLoss: 0.058622\nTrain Epoch: 18 [40000/60000 (67%)]\tLoss: 0.064776\nTrain Epoch: 18 [50000/60000 (83%)]\tLoss: 0.034212\n\nTest set: Average loss: 0.3250, Accuracy: 9183/10000 (91.8300%)\n\nTrain Epoch: 19 [0/60000 (0%)]\tLoss: 0.164495\nTrain Epoch: 19 [10000/60000 (17%)]\tLoss: 0.040864\nTrain Epoch: 19 [20000/60000 (33%)]\tLoss: 0.059513\nTrain Epoch: 19 [30000/60000 (50%)]\tLoss: 0.037228\nTrain Epoch: 19 [40000/60000 (67%)]\tLoss: 0.034136\nTrain Epoch: 19 [50000/60000 (83%)]\tLoss: 0.043602\n\nTest set: Average loss: 0.3246, Accuracy: 9145/10000 (91.4500%)\n\nTrain Epoch: 20 [0/60000 (0%)]\tLoss: 0.031372\nTrain Epoch: 20 [10000/60000 (17%)]\tLoss: 0.043065\nTrain Epoch: 20 [20000/60000 (33%)]\tLoss: 0.096568\nTrain Epoch: 20 [30000/60000 (50%)]\tLoss: 0.040438\nTrain Epoch: 20 [40000/60000 (67%)]\tLoss: 0.086414\nTrain Epoch: 20 [50000/60000 (83%)]\tLoss: 0.068247\n\nTest set: Average loss: 0.3635, Accuracy: 9168/10000 (91.6800%)\n\nTrain Epoch: 21 [0/60000 (0%)]\tLoss: 0.054272\nTrain Epoch: 21 [10000/60000 (17%)]\tLoss: 0.016925\nTrain Epoch: 21 [20000/60000 (33%)]\tLoss: 0.012585\nTrain Epoch: 21 [30000/60000 (50%)]\tLoss: 0.089346\nTrain Epoch: 21 [40000/60000 (67%)]\tLoss: 0.023714\nTrain Epoch: 21 [50000/60000 (83%)]\tLoss: 0.055778\n\nTest set: Average loss: 0.3602, Accuracy: 9133/10000 (91.3300%)\n\nTrain Epoch: 22 [0/60000 (0%)]\tLoss: 0.033472\nTrain Epoch: 22 [10000/60000 (17%)]\tLoss: 0.043975\nTrain Epoch: 22 [20000/60000 (33%)]\tLoss: 0.011037\nTrain Epoch: 22 [30000/60000 (50%)]\tLoss: 0.020694\nTrain Epoch: 22 [40000/60000 (67%)]\tLoss: 0.026448\nTrain Epoch: 22 [50000/60000 (83%)]\tLoss: 0.032209\n\nTest set: Average loss: 0.3590, Accuracy: 9196/10000 (91.9600%)\n\nTrain Epoch: 23 [0/60000 (0%)]\tLoss: 0.058136\nTrain Epoch: 23 [10000/60000 (17%)]\tLoss: 0.013657\nTrain Epoch: 23 [20000/60000 (33%)]\tLoss: 0.021356\nTrain Epoch: 23 [30000/60000 (50%)]\tLoss: 0.015538\nTrain Epoch: 23 [40000/60000 (67%)]\tLoss: 0.036618\nTrain Epoch: 23 [50000/60000 (83%)]\tLoss: 0.057177\n\nTest set: Average loss: 0.4228, Accuracy: 9189/10000 (91.8900%)\n\nTrain Epoch: 24 [0/60000 (0%)]\tLoss: 0.030148\nTrain Epoch: 24 [10000/60000 (17%)]\tLoss: 0.009769\nTrain Epoch: 24 [20000/60000 (33%)]\tLoss: 0.004893\nTrain Epoch: 24 [30000/60000 (50%)]\tLoss: 0.106669\nTrain Epoch: 24 [40000/60000 (67%)]\tLoss: 0.074532\nTrain Epoch: 24 [50000/60000 (83%)]\tLoss: 0.032662\n\nTest set: Average loss: 0.3959, Accuracy: 9193/10000 (91.9300%)\n\nTrain Epoch: 25 [0/60000 (0%)]\tLoss: 0.039011\nTrain Epoch: 25 [10000/60000 (17%)]\tLoss: 0.019157\nTrain Epoch: 25 [20000/60000 (33%)]\tLoss: 0.048471\nTrain Epoch: 25 [30000/60000 (50%)]\tLoss: 0.021187\nTrain Epoch: 25 [40000/60000 (67%)]\tLoss: 0.043939\nTrain Epoch: 25 [50000/60000 (83%)]\tLoss: 0.010241\n\nTest set: Average loss: 0.4070, Accuracy: 9180/10000 (91.8000%)\n\nTrain Epoch: 26 [0/60000 (0%)]\tLoss: 0.073690\nTrain Epoch: 26 [10000/60000 (17%)]\tLoss: 0.015607\nTrain Epoch: 26 [20000/60000 (33%)]\tLoss: 0.022162\nTrain Epoch: 26 [30000/60000 (50%)]\tLoss: 0.030061\nTrain Epoch: 26 [40000/60000 (67%)]\tLoss: 0.027323\nTrain Epoch: 26 [50000/60000 (83%)]\tLoss: 0.015188\n\nTest set: Average loss: 0.4887, Accuracy: 9105/10000 (91.0500%)\n\nTrain Epoch: 27 [0/60000 (0%)]\tLoss: 0.003502\nTrain Epoch: 27 [10000/60000 (17%)]\tLoss: 0.029720\nTrain Epoch: 27 [20000/60000 (33%)]\tLoss: 0.114730\nTrain Epoch: 27 [30000/60000 (50%)]\tLoss: 0.040344\nTrain Epoch: 27 [40000/60000 (67%)]\tLoss: 0.038422\nTrain Epoch: 27 [50000/60000 (83%)]\tLoss: 0.010868\n\nTest set: Average loss: 0.4717, Accuracy: 9141/10000 (91.4100%)\n\nTrain Epoch: 28 [0/60000 (0%)]\tLoss: 0.009812\nTrain Epoch: 28 [10000/60000 (17%)]\tLoss: 0.055452\nTrain Epoch: 28 [20000/60000 (33%)]\tLoss: 0.014152\nTrain Epoch: 28 [30000/60000 (50%)]\tLoss: 0.011011\nTrain Epoch: 28 [40000/60000 (67%)]\tLoss: 0.041616\nTrain Epoch: 28 [50000/60000 (83%)]\tLoss: 0.042724\n\nTest set: Average loss: 0.4967, Accuracy: 9164/10000 (91.6400%)\n\nTrain Epoch: 29 [0/60000 (0%)]\tLoss: 0.038814\nTrain Epoch: 29 [10000/60000 (17%)]\tLoss: 0.005466\nTrain Epoch: 29 [20000/60000 (33%)]\tLoss: 0.041350\nTrain Epoch: 29 [30000/60000 (50%)]\tLoss: 0.004931\nTrain Epoch: 29 [40000/60000 (67%)]\tLoss: 0.015262\nTrain Epoch: 29 [50000/60000 (83%)]\tLoss: 0.023740\n\nTest set: Average loss: 0.5055, Accuracy: 9165/10000 (91.6500%)\n\nTrain Epoch: 30 [0/60000 (0%)]\tLoss: 0.024831\nTrain Epoch: 30 [10000/60000 (17%)]\tLoss: 0.013454\nTrain Epoch: 30 [20000/60000 (33%)]\tLoss: 0.014403\nTrain Epoch: 30 [30000/60000 (50%)]\tLoss: 0.038219\nTrain Epoch: 30 [40000/60000 (67%)]\tLoss: 0.040868\nTrain Epoch: 30 [50000/60000 (83%)]\tLoss: 0.017028\n\nTest set: Average loss: 0.5028, Accuracy: 9200/10000 (92.0000%)\n\n","name":"stdout"}],"source":["torch.manual_seed(2018)  # 设置随机数种子\n","model = LeNet5()  # 实例化模型\n","#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) \n","optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n","#optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.97, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n","#optimizer=optim.Adadelta(model.parameters(), lr=1.0, rho=0.95, eps=1e-06, weight_decay=0)\n","n_epoch = 30  # 修改迭代更多次\n","for epoch in range(1, n_epoch + 1):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader): #枚举函数，枚举数据集中的数据\n","        optimizer.zero_grad()  # 梯度清空\n","        output = model(data)  # 前向推理\n","        #print(output)\n","        #print(batch_idx)\n","        loss = F.cross_entropy(output, target) # 交叉熵损失函数\n","        #loss = F.nll_loss(output, target)  # 非负极大似然损失函数\n","        loss.backward()  # 反向传播计算梯度\n","        optimizer.step()  # 参数调整\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","\n","    model.eval()  # 测试模式\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            output = model(data)\n","            test_loss += F.cross_entropy(\n","                output, target, size_average=False).item()  # 计算每一组的loss\n","            #test_loss += F.nll_loss(\n","                #output, target, size_average=False).item()  # 计算每一组的loss\n","            pred = output.max(\n","                1, keepdim=True)[1]  # 获取索引\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    print(\n","        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n","            test_loss, correct, len(test_loader.dataset),\n","            100. * correct / len(test_loader.dataset)))"],"hide_input":false},{"cell_type":"markdown","metadata":{"id":"9478756FA5B840FE8620F775ACC28EBB","mdEditEnable":false},"source":["4.提取特征"],"hide_input":false,"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"B23C4A0988024887AD8B4DA463B1BF60","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"FeatureExtractor(\n  (submodule): LeNet5(\n    (C1): Sequential(\n      (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n      (1): ReLU()\n    )\n    (S2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (C3): Sequential(\n      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n      (1): ReLU()\n    )\n    (S4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (C5): Sequential(\n      (0): Linear(in_features=1600, out_features=256, bias=True)\n      (1): ReLU()\n    )\n    (F6): Sequential(\n      (0): Linear(in_features=256, out_features=128, bias=True)\n      (1): ReLU()\n    )\n    (OUT): Linear(in_features=128, out_features=16, bias=True)\n  )\n)\n","name":"stdout"}],"source":["# 创建提取特征类\n","class FeatureExtractor(nn.Module):\n","    def __init__(self, submodule, extracted_layers):\n","        super(FeatureExtractor, self).__init__()\n","        self.submodule = submodule\n","        self.extracted_layers = extracted_layers\n","\n","    def forward(self, x):\n","        outputs = []\n","        for name, module in self.submodule._modules.items():\n","            if name is \"C5\":\n","                x = x.view(x.size(0), -1)\n","            x = module(x)\n","            if name in self.extracted_layers:\n","                outputs.append(x)\n","        return outputs\n","print(FeatureExtractor(model,['C1','C2']))"],"hide_input":false},{"cell_type":"code","execution_count":13,"metadata":{"id":"5F6C150D388F4A8DBD0EA8B944FF82C7","collapsed":false,"scrolled":true},"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 720x720 with 6 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/5F6C150D388F4A8DBD0EA8B944FF82C7/patgke8s8o.png\">"}},{"output_type":"display_data","metadata":{},"data":{"text/plain":"<Figure size 720x720 with 16 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/5F6C150D388F4A8DBD0EA8B944FF82C7/patgke6gls.png\">"}}],"source":["# 提取数据\n","test_loader_for_feature = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n","img, label = iter(test_loader_for_feature).next()\n","exactor = FeatureExtractor(model, ['C1', 'C3'])\n","features = exactor(img)\n","# 可视化\n","pylab.rcParams['figure.figsize'] = (10.0, 10.0) # plot size\n","for i in range(6):\n","    ax = plt.subplot(2, 3, i + 1)\n","    ax.set_title('Sample #{}'.format(i))\n","    ax.axis('off')\n","    plt.imshow(features[0].data.numpy()[0, i, :, :], cmap='jet')\n","plt.show()\n","for i in range(16):\n","    ax = plt.subplot(4, 4, i + 1)\n","    ax.set_title('Sample #{}'.format(i))\n","    ax.axis('off')\n","    plt.imshow(features[1].data.numpy()[0, i, :, :], cmap='jet')\n","plt.show()"],"hide_input":false},{"cell_type":"markdown","execution_count":null,"metadata":{"id":"815D526282D9422193BEFA7B90E9FD58","mdEditEnable":false},"source":["**采用基本和lenet5相似的网络结构，第一层有32个55的特征提取器，然后进行22的池化，然后使用64个33的特征提取器，然后进行22的池化，这时维度计算公式为（（（（28-5+1）/2）-3+1）/2）^264，即1600维，然后经历256、128两个全连接层提取出16个特征，之后利用交叉熵损失函数和adam优化算法，迭代30次之后得到92%的准确率。**"],"hide_input":false,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":2}
